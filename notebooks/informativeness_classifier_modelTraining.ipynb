{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1lpQivU04xNyXosfNJiDOFZ5G7TtY6tKI","authorship_tag":"ABX9TyPwaNrq9zQBbKTAR8u3bS3G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XbGsmQeXxbZ1","executionInfo":{"status":"ok","timestamp":1739637280806,"user_tz":-60,"elapsed":635,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}},"outputId":"9cbd4328-b175-4513-9545-21e495174b4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler\n","from torch.optim import AdamW\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","\n","# Cargar el dataset\n","df = pd.read_csv(\"/disaster_preprocessed.csv\")\n","\n","\n","# Verificar si GPU está disponible y configurar el dispositivo\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device:\", device)\n"]},{"cell_type":"code","source":["# Mapear etiquetas de `Informativeness_label`\n","label_mapping = {\n","    'informative': 1,\n","    'not related or not informative': 0\n","}\n","df['Informativeness_label_mapped'] = df['Informativeness_label'].map(label_mapping)\n","df = df.dropna(subset=['Informativeness_label_mapped'])"],"metadata":{"id":"WbkP1hioxpqR","executionInfo":{"status":"ok","timestamp":1739637283603,"user_tz":-60,"elapsed":175,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# División inicial: Training (80%), Validation (5000), Test (5000)\n","train_df, temp_df = train_test_split(df, test_size=10000, random_state=42, stratify=df['Informativeness_label_mapped'])\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['Informativeness_label_mapped'])\n","\n","print(f\"Tamaño de training: {len(train_df)}, validación: {len(val_df)}, prueba: {len(test_df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxYNi6Y_xpnz","executionInfo":{"status":"ok","timestamp":1739637285417,"user_tz":-60,"elapsed":181,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}},"outputId":"25b9f489-633c-4dc7-b6a9-3dc0ed586d3c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño de training: 64346, validación: 5000, prueba: 5000\n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","# Tokenizer preentrenado\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"metadata":{"id":"YX6NrC_Pdyt1","executionInfo":{"status":"ok","timestamp":1739637369346,"user_tz":-60,"elapsed":183,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Codificación de etiquetas\n","train_labels = train_df['Informativeness_label_mapped'].values\n","val_labels = val_df['Informativeness_label_mapped'].values\n","test_labels = test_df['Informativeness_label_mapped'].values"],"metadata":{"id":"PIPLNLyGesC8","executionInfo":{"status":"ok","timestamp":1739638127201,"user_tz":-60,"elapsed":176,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Tokenización\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Dataset compatible con PyTorch\n","class DisasterDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        encoding = self.tokenizer(\n","            self.texts[idx],\n","            padding='max_length',\n","            truncation=True,\n","            max_length=128,\n","            return_tensors='pt'\n","        )\n","        return {key: val.squeeze() for key, val in encoding.items()}, torch.tensor(self.labels[idx])\n","\n","# Crear datasets y DataLoaders\n","train_dataset = DisasterDataset(list(train_df['ProcessedText']), train_labels, tokenizer)\n","val_dataset = DisasterDataset(list(val_df['ProcessedText']), val_labels, tokenizer)\n","test_dataset = DisasterDataset(list(test_df['ProcessedText']), test_labels, tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32)\n","test_loader = DataLoader(test_dataset, batch_size=32)"],"metadata":{"id":"ot4T-nfcxplg","executionInfo":{"status":"ok","timestamp":1739638130163,"user_tz":-60,"elapsed":354,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["print(train_df.columns)\n","print(train_df['Informativeness_label_mapped'].value_counts())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKeDN3-gi-GD","executionInfo":{"status":"ok","timestamp":1739638137045,"user_tz":-60,"elapsed":164,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}},"outputId":"18dfac61-db94-4e77-c3c3-1c6c292ec69c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['InformationType', 'event', 'TweetID', 'TweetText', 'location', 'year',\n","       'Informativeness_label', 'ProcessedText', 'ProcessedText_length',\n","       'Informativeness_label_mapped'],\n","      dtype='object')\n","Informativeness_label_mapped\n","1    37684\n","0    26662\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","source":["# Función de evaluación\n","def evaluate(loader, model):\n","    model.eval()\n","    predictions, true_labels = [], []\n","    total_loss = 0\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            batch_inputs, batch_labels = batch\n","            batch_inputs = {key: val.to(device) for key, val in batch_inputs.items()}\n","            batch_labels = batch_labels.to(device)\n","\n","            outputs = model(**batch_inputs)\n","            loss = loss_fn(outputs.logits, batch_labels)\n","            total_loss += loss.item()\n","\n","            predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n","            true_labels.extend(batch_labels.cpu().numpy())\n","\n","    return total_loss / len(loader), classification_report(true_labels, predictions, digits=3)\n","\n","# Early Stopping y Guardar el mejor modelo\n","early_stopping_patience = 2\n","best_val_loss = float('inf')\n","early_stopping_counter = 0\n","\n","# Cross-Validation con StratifiedKFold\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","fold_results = []\n","\n","for fold, (train_index, val_index) in enumerate(skf.split(train_df['ProcessedText'], train_df['Informativeness_label_mapped'])):\n","    train_texts = train_df.iloc[train_index]['ProcessedText'].values\n","    val_texts = train_df.iloc[val_index]['ProcessedText'].values\n","    train_labels_fold = train_df.iloc[train_index]['Informativeness_label_mapped'].values\n","    val_labels_fold = train_df.iloc[val_index]['Informativeness_label_mapped'].values\n","\n","    train_dataset_fold = DisasterDataset(train_texts, train_labels_fold, tokenizer)\n","    val_dataset_fold = DisasterDataset(val_texts, val_labels_fold, tokenizer)\n","\n","    train_loader_fold = DataLoader(train_dataset_fold, batch_size=32, shuffle=True)\n","    val_loader_fold = DataLoader(val_dataset_fold, batch_size=32)\n","\n","    # Modelo BERT para este fold\n","    model = BertForSequenceClassification.from_pretrained(\n","        'bert-base-uncased',\n","        num_labels=2,\n","        hidden_dropout_prob=0.3\n","    )\n","    model.to(device)\n","\n","    # Optimización y Scheduler\n","    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","    scheduler = get_scheduler(\n","        'linear',\n","        optimizer=optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=len(train_loader_fold) * 3\n","    )\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    # Entrenamiento con Early Stopping\n","    for epoch in range(10):  # Hasta 10 épocas, pero con Early Stopping\n","        model.train()\n","        total_loss = 0\n","        for batch in train_loader_fold:\n","            batch_inputs, batch_labels = batch\n","            batch_inputs = {key: val.to(device) for key, val in batch_inputs.items()}\n","            batch_labels = batch_labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(**batch_inputs, labels=batch_labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","        # Evaluación\n","        val_loss, val_report = evaluate(val_loader_fold, model)\n","        print(f\"Epoch {epoch + 1} - Training Loss: {total_loss / len(train_loader_fold)}, Validation Loss: {val_loss}\")\n","\n","        # Guardar el mejor modelo\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), f\"best_model_fold_{fold + 1}.pt\")\n","            early_stopping_counter = 0\n","            print(\" Mejor modelo guardado\")\n","        else:\n","            early_stopping_counter += 1\n","            print(f\"Early Stopping Counter: {early_stopping_counter}/{early_stopping_patience}\")\n","\n","        # Verificar Early Stopping\n","        if early_stopping_counter >= early_stopping_patience:\n","            print(\" Early stopping activado.\")\n","            break\n","\n","    fold_results.append(best_val_loss)\n","\n","# Promedio de validación cruzada\n","print(f\"\\n📊 Cross-validation average loss: {np.mean(fold_results):.4f} ± {np.std(fold_results):.4f}\")\n","\n","# Evaluación final en el conjunto de prueba\n","model.load_state_dict(torch.load(\"best_model_fold_1.pt\"))\n","test_loss, test_report = evaluate(test_loader, model)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Classification Report:\\n{test_report}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cl6Kg336cYuf","executionInfo":{"status":"ok","timestamp":1739640910042,"user_tz":-60,"elapsed":2650742,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}},"outputId":"67567d2b-1862-4b94-8ba2-d79472554e2b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 - Training Loss: 0.4018289534070612, Validation Loss: 0.36914002468287205\n"," Mejor modelo guardado\n","Epoch 2 - Training Loss: 0.33523269796441846, Validation Loss: 0.36497097991166577\n"," Mejor modelo guardado\n","Epoch 3 - Training Loss: 0.30244950238890794, Validation Loss: 0.3686865176219798\n","Early Stopping Counter: 1/2\n","Epoch 4 - Training Loss: 0.291620808283667, Validation Loss: 0.3686865176219798\n","Early Stopping Counter: 2/2\n"," Early stopping activado.\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 - Training Loss: 0.40335273499108015, Validation Loss: 0.3859610794199903\n","Early Stopping Counter: 3/2\n"," Early stopping activado.\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 - Training Loss: 0.4003746763214404, Validation Loss: 0.390299836048861\n","Early Stopping Counter: 4/2\n"," Early stopping activado.\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 - Training Loss: 0.40340957499350577, Validation Loss: 0.3810268862001949\n","Early Stopping Counter: 5/2\n"," Early stopping activado.\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 - Training Loss: 0.4053020338192721, Validation Loss: 0.3429148486323155\n"," Mejor modelo guardado\n","Epoch 2 - Training Loss: 0.336424826056858, Validation Loss: 0.3513126565564063\n","Early Stopping Counter: 1/2\n","Epoch 3 - Training Loss: 0.30623411803029943, Validation Loss: 0.3669780609902762\n","Early Stopping Counter: 2/2\n"," Early stopping activado.\n","\n","📊 Cross-validation average loss: 0.3606 ± 0.0088\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-12-c2ae3360454d>:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(\"best_model_fold_1.pt\"))\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.3664361433523476\n","Test Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0      0.881     0.751     0.811      2072\n","           1      0.841     0.928     0.882      2928\n","\n","    accuracy                          0.855      5000\n","   macro avg      0.861     0.840     0.847      5000\n","weighted avg      0.857     0.855     0.853      5000\n","\n"]}]},{"cell_type":"code","source":["# Modelo\n","model.save_pretrained(\"bert_informativeness_classifier\")\n","\n","# Tokenizer\n","tokenizer.save_pretrained(\"bert_informativeness_classifier\")\n","\n","print(\"Modelo y tokenizer guardados en 'bert_event_classifier'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ESh3kYHbruB2","executionInfo":{"status":"ok","timestamp":1739641584087,"user_tz":-60,"elapsed":1450,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}},"outputId":"f4e187c2-0874-4d0d-b0bf-43e09d08a109"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Modelo y tokenizer guardados en 'bert_event_classifier'\n"]}]},{"cell_type":"code","source":["import joblib\n","\n","# Guardar el diccionario como archivo pickle\n","joblib.dump(label_mapping, 'label_mapping.pkl')\n","print(\"Diccionario de mapeo guardado como label_mapping.pkl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bKzBpF6kr37v","executionInfo":{"status":"ok","timestamp":1739641585181,"user_tz":-60,"elapsed":273,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}},"outputId":"68616382-3fea-4a52-e19f-676834ade1e2"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Diccionario de mapeo guardado como label_mapping.pkl\n"]}]},{"cell_type":"code","source":["!zip -r bert_informativeness_classifier.zip bert_informativeness_classifier\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GSooXZ88kbYs","executionInfo":{"status":"ok","timestamp":1739641609594,"user_tz":-60,"elapsed":24415,"user":{"displayName":"Luis Andres Oyervides","userId":"04755987859605504594"}},"outputId":"ded9baea-9ca8-41ff-f76d-c97628bbc75c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: bert_informativeness_classifier/ (stored 0%)\n","  adding: bert_informativeness_classifier/config.json (deflated 49%)\n","  adding: bert_informativeness_classifier/tokenizer_config.json (deflated 75%)\n","  adding: bert_informativeness_classifier/model.safetensors (deflated 7%)\n","  adding: bert_informativeness_classifier/special_tokens_map.json (deflated 42%)\n","  adding: bert_informativeness_classifier/vocab.txt (deflated 53%)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OdU3WrIgt_zK"},"execution_count":null,"outputs":[]}]}