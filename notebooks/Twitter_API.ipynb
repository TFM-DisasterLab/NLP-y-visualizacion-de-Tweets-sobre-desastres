{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAjjNCoks8vq"
   },
   "source": [
    "## Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 4223,
     "status": "ok",
     "timestamp": 1739655797866,
     "user": {
      "displayName": "Gerardo Alemán",
      "userId": "15486245495058526554"
     },
     "user_tz": -60
    },
    "id": "Fl9e3RzGif0N",
    "outputId": "887d893a-2f01-49d1-f93e-6883cae1502b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token cargado correctamente.\n",
      "IDs ya procesados: {'1890480025100529825', '1890087351789207867', '1889775381479067830', '1890013345568436246', '1891589156624601263', '1889766982116868244', '1889996464216244560', '1889476378740793393', '1889797904979915119', '1891405147302990126', '1889946766810009752', '1890414131649372246', '1891588644013572331'}\n",
      "\n",
      "✅ Tweets guardados en 'tweets.csv'\n",
      "IDs procesados actualizados.\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import os\n",
    "import IPython.display as display  # Para mostrar tablas\n",
    "\n",
    "# Leer token.txt\n",
    "token_path = \"bearer_token.txt\"\n",
    "try:\n",
    "    with open(token_path, \"r\") as f:\n",
    "        BEARER_TOKEN = f.read().strip()\n",
    "    print(\"Token cargado correctamente.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"\\u26a0\\ufe0f No se encontró el archivo '{token_path}'. Asegúrate de haberlo guardado antes.\")\n",
    "\n",
    "# Crear el cliente de Tweepy\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
    "\n",
    "# Definir la consulta\n",
    "query = \"(hurricane OR earthquake OR flood OR tornado OR tsunami OR wildfire OR landslide OR avalanche OR typhoon OR cyclone OR eruption OR blizzard) -is:retweet lang:en\"\n",
    "\n",
    "# Archivo que almacenará los IDs de tweets ya procesados\n",
    "processed_ids_file = \"processed_ids.txt\"\n",
    "if os.path.exists(processed_ids_file):\n",
    "    with open(processed_ids_file, \"r\") as f:\n",
    "        processed_ids = {line.strip() for line in f if line.strip()}\n",
    "    print(f\"IDs ya procesados: {processed_ids}\")\n",
    "else:\n",
    "    processed_ids = set()\n",
    "    print(\"No se encontró archivo de IDs procesados, se inicia con un conjunto vacío.\")\n",
    "\n",
    "# Realizar la búsqueda de tweets recientes sin usar 'since_id'\n",
    "response = client.search_recent_tweets(\n",
    "    query=query,\n",
    "    tweet_fields=[\"author_id\", \"created_at\", \"id\", \"geo\"],\n",
    "    user_fields=[\"location\", \"verified\"],\n",
    "    expansions=[\"author_id\"],\n",
    "    max_results=100\n",
    ")\n",
    "\n",
    "if response.data:\n",
    "    # Filtrar solo los tweets cuyos IDs no estén ya procesados\n",
    "    new_tweets = [tweet for tweet in response.data if str(tweet.id) not in processed_ids]\n",
    "\n",
    "    if new_tweets:\n",
    "        # Obtener datos de los usuarios si están disponibles\n",
    "        users = {user.id: user for user in response.includes[\"users\"]} if response.includes else {}\n",
    "\n",
    "        # Preparar la información de cada tweet\n",
    "        tweet_records = []\n",
    "        for tweet in new_tweets:\n",
    "            record = {\n",
    "                \"TweetID\": tweet.id,\n",
    "                \"AuthorID\": tweet.author_id,\n",
    "                \"CreatedAt\": tweet.created_at.isoformat() if tweet.created_at else \"\",\n",
    "                \"Location\": (users.get(tweet.author_id, {}).location \n",
    "                             if tweet.author_id in users and hasattr(users[tweet.author_id], \"location\") \n",
    "                             else \"Desconocida\"),\n",
    "                \"Country\": (users.get(tweet.author_id, {}).geo \n",
    "                            if tweet.author_id in users and hasattr(users[tweet.author_id], \"geo\") \n",
    "                            else \"Desconocido\"),\n",
    "                \"Description\": (users[tweet.author_id].description \n",
    "                                if tweet.author_id in users and hasattr(users[tweet.author_id], \"description\")\n",
    "                                else \"No disponible\"),\n",
    "                \"Verified\": (users.get(tweet.author_id, {}).verified \n",
    "                             if tweet.author_id in users and hasattr(users[tweet.author_id], \"verified\") \n",
    "                             else \"Desconocido\"),\n",
    "                \"Text\": tweet.text\n",
    "            }\n",
    "            tweet_records.append(record)\n",
    "\n",
    "        df_new = pd.DataFrame(tweet_records)\n",
    "\n",
    "        tweets_file = \"tweets.csv\"\n",
    "\n",
    "        # Concatenar nuevos tweets si ya existe el archivo\n",
    "        if os.path.exists(tweets_file):\n",
    "            df_existing = pd.read_csv(tweets_file)\n",
    "            df_total = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        else:\n",
    "            df_total = df_new\n",
    "\n",
    "        # Guardar (o actualizar) el archivo CSV\n",
    "        df_total.to_csv(tweets_file, index=False)\n",
    "        print(f\"\\n\\u2705 Tweets guardados en '{tweets_file}'\")\n",
    "\n",
    "        # Actualizar el archivo de IDs con los nuevos tweets procesados\n",
    "        with open(processed_ids_file, \"a\") as f:\n",
    "            for tweet in new_tweets:\n",
    "                tweet_id_str = str(tweet.id)\n",
    "                if tweet_id_str not in processed_ids:\n",
    "                    f.write(tweet_id_str + \"\\n\")\n",
    "                    processed_ids.add(tweet_id_str)\n",
    "        print(\"IDs procesados actualizados.\")\n",
    "    else:\n",
    "        print(\"No hay nuevos tweets para agregar (todos ya fueron registrados).\")\n",
    "else:\n",
    "    print(\"No se encontraron tweets con la consulta indicada.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
